{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP1RYlSqvs8HVw+ASl1vpHO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"754oZBn9uhyy","executionInfo":{"status":"ok","timestamp":1765778139208,"user_tz":360,"elapsed":41618,"user":{"displayName":"Fangzhou Du","userId":"14946545626395308065"}},"outputId":"094d6132-083e-4bd4-c732-05d50e03c5c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Using Colab cache for faster access to the 'plantdisease' dataset.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms, models\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import Subset\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","from pathlib import Path\n","from matplotlib.patches import Patch\n","from matplotlib.colors import ListedColormap, BoundaryNorm\n","import random\n","from torch.utils.data import Subset as TorchSubset\n","\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","SAVE_DIR = '/content/drive/MyDrive/comp576final'\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","import kagglehub\n","path = kagglehub.dataset_download(\"emmarex/plantdisease\")\n","data_dir = os.path.join(path, 'PlantVillage')\n","\n","def create_resnet18_baseline(num_classes, pretrained=True):\n","    model = models.resnet18(pretrained=pretrained)\n","    num_ftrs = model.fc.in_features\n","    model.fc = nn.Linear(num_ftrs, num_classes)\n","    return model\n","\n","class ResNet18Plant(nn.Module):\n","    def __init__(self, num_classes, pretrained=True):\n","        super().__init__()\n","        if pretrained:\n","            backbone = models.resnet18(weights=\"IMAGENET1K_V1\")\n","        else:\n","            backbone = models.resnet18(weights=None)\n","\n","        self.features = nn.Sequential(*list(backbone.children())[:-2])\n","        in_channels = 512\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(in_channels * 2, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        gap = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)\n","        gmp = F.adaptive_max_pool2d(x, 1).view(x.size(0), -1)\n","        feat = torch.cat([gap, gmp], dim=1)\n","        out = self.classifier(feat)\n","        return out\n","\n","class CustomCNN(nn.Module):\n","    def __init__(self, num_classes, img_size=224):\n","        super().__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),\n","        )\n","        self.global_pool = nn.AdaptiveAvgPool2d(1)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(256, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.global_pool(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return out\n","\n","def get_prediction_colormap(num_classes):\n","    base_cmap = plt.get_cmap('tab20')\n","    colors = [base_cmap(i % base_cmap.N) for i in range(num_classes)]\n","    cmap = ListedColormap(colors)\n","    boundaries = np.arange(-0.5, num_classes + 0.5, 1.0)\n","    norm = BoundaryNorm(boundaries, ncolors=num_classes)\n","    return cmap, norm\n","\n","def denormalize_image(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n","    tensor = tensor.clone()\n","    for t, m, s in zip(tensor, mean, std):\n","        t.mul_(s).add_(m)\n","    return tensor\n","\n","def occlusion_sensitivity_analysis(models_info, device, loader, output_dir, class_names,\n","                                   num_examples=3, patch_size=16, stride=8, baseline_value=0.0):\n","    output_dir = Path(output_dir)\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","    samples = []\n","    for data, target in loader:\n","        samples.append((data, int(target.item())))\n","        if len(samples) >= num_examples:\n","            break\n","    for sample_idx, (image_cpu, label_idx) in enumerate(samples):\n","        # Ensure image_cpu has batch dimension\n","        if image_cpu.dim() == 3:\n","            image_cpu = image_cpu.unsqueeze(0)\n","        image = image_cpu.to(device)\n","        # Calculate occlusion grid\n","        _, _, height, width = image_cpu.shape\n","        y_positions = list(range(0, height - patch_size + 1, stride))\n","        x_positions = list(range(0, width - patch_size + 1, stride))\n","        # cover the edges\n","        if y_positions[-1] + patch_size < height:\n","            y_positions.append(height - patch_size)\n","        if x_positions[-1] + patch_size < width:\n","            x_positions.append(width - patch_size)\n","        grid_y = len(y_positions)\n","        grid_x = len(x_positions)\n","\n","        fig, axes = plt.subplots(1, 6, figsize=(30, 5))\n","        ax_orig = axes[0]\n","        image_display_orig = denormalize_image(image_cpu[0]).cpu().numpy()\n","        image_display_orig = np.transpose(image_display_orig, (1, 2, 0))\n","        image_display_orig = np.clip(image_display_orig, 0.0, 1.0)\n","        ax_orig.imshow(image_display_orig)\n","        ax_orig.set_title(f'Original Image\\nTrue Label: {class_names[label_idx]}',\n","                         fontsize=12, fontweight='bold')\n","        ax_orig.axis('off')\n","\n","        for model_idx, (model, model_name) in enumerate(models_info):\n","            model.eval()\n","            with torch.no_grad():\n","                output_base = model(image)\n","                probas_base = F.softmax(output_base, dim=1)\n","                base_prob = probas_base[0, label_idx].item()\n","                base_pred = output_base.argmax(dim=1).item()\n","\n","                prob_map = np.full((grid_y, grid_x), np.nan, dtype=np.float32)\n","                for iy, y0 in enumerate(y_positions):\n","                    for ix, x0 in enumerate(x_positions):\n","                        occluded = image_cpu.clone()\n","                        y_end = min(y0 + patch_size, height)\n","                        x_end = min(x0 + patch_size, width)\n","                        if isinstance(baseline_value, torch.Tensor):\n","                            occluded[:, :, y0:y_end, x0:x_end] = baseline_value.view(3, 1, 1)\n","                        else:\n","                            occluded[:, :, y0:y_end, x0:x_end] = baseline_value\n","                        occluded_device = occluded.to(device)\n","                        output_occ = model(occluded_device)\n","                        probas_occ = F.softmax(output_occ, dim=1)\n","                        prob_map[iy, ix] = probas_occ[0, label_idx].item()\n","\n","            ax = axes[model_idx + 1]\n","            image_display = denormalize_image(image_cpu[0]).cpu().numpy()\n","            image_display = np.transpose(image_display, (1, 2, 0))\n","            image_display = np.clip(image_display, 0.0, 1.0)\n","            prob_drop = base_prob - prob_map\n","            max_drop = np.nanmax(prob_drop)\n","            ax.imshow(image_display)\n","            im_overlay = ax.imshow(prob_drop, cmap='hot', origin='upper',\n","                                  extent=[0, width, height, 0], interpolation='bilinear',\n","                                  vmin=0, vmax=1.0, alpha=0.6)\n","            correct_mark = \"correct\" if base_pred == label_idx else \"incorrect\"\n","            ax.set_title(f'{model_name} {correct_mark}\\nPred: {class_names[base_pred]} ({base_prob:.1%})\\nMax Drop: {max_drop:.4f}',\n","                        fontsize=10, fontweight='bold')\n","            ax.axis('off')\n","\n","            plt.colorbar(im_overlay, ax=ax, fraction=0.046, pad=0.04, label='Prob. Drop')\n","        fig.suptitle(f'Occlusion Sensitivity Comparison - Sample {sample_idx + 1}\\n'\n","                    f'Patch: {patch_size}*{patch_size}, Stride: {stride}',\n","                    fontsize=14, fontweight='bold')\n","        fig.tight_layout(rect=[0, 0.01, 1, 0.96])\n","        output_path = output_dir / f'occlusion_patch{patch_size}_sample_{sample_idx + 1:02d}.png'\n","        fig.savefig(output_path, dpi=200, bbox_inches='tight')\n","        plt.close(fig)\n","\n","\n","def main():\n","    raw_dataset = ImageFolder(root=data_dir)\n","    num_classes = len(raw_dataset.classes)\n","    class_names = raw_dataset.classes\n","    split_path = os.path.join(SAVE_DIR, 'data_split_indices.pth')\n","    split_indices = torch.load(split_path)\n","    test_indices = split_indices['test_indices']\n","    test_subset = Subset(raw_dataset, test_indices)\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","    output_dir = Path(SAVE_DIR) / 'occlusion_sensitivity'\n","    output_dir.mkdir(exist_ok=True)\n","\n","    num_samples = 10\n","    patch_size = 56\n","    stride = 28\n","\n","    baseline_tensor = torch.randn(3, 1, 1) * 2.0  # Random noise with large std\n","    sampled_indices = np.random.choice(len(test_subset), num_samples, replace=False)\n","    selected_samples = TorchSubset(test_subset, sampled_indices.tolist())\n","\n","    class TransformedSubset:\n","        def __init__(self, subset, transform):\n","            self.subset = subset\n","            self.transform = transform\n","\n","        def __len__(self):\n","            return len(self.subset)\n","\n","        def __getitem__(self, idx):\n","            image, label = self.subset[idx]\n","            return self.transform(image), label\n","\n","    transformed_samples = TransformedSubset(selected_samples, transform)\n","    sample_loader = torch.utils.data.DataLoader(transformed_samples, batch_size=1, shuffle=False)\n","    models_info = []\n","    models_to_analyze = [\n","        ('ResNet18 Baseline', create_resnet18_baseline, 'best_baseline_resnet18.pth'),\n","        ('ResNet18 Improved', ResNet18Plant, 'best_improved_resnet18plant.pth'),\n","        ('CNN Scratch', CustomCNN, 'best_cnn_model.pth'),\n","        ('CNN Distilled (Baseline)', CustomCNN, 'best_customcnn_distilled_real_baseline.pth'),\n","        ('CNN Distilled (Improved)', CustomCNN, 'best_customcnn_distilled.pth'),\n","    ]\n","\n","    for model_name, model_class, checkpoint_name in models_to_analyze:\n","        if 'ResNet18 Baseline' in model_name:\n","            model = model_class(num_classes, pretrained=False)\n","        elif 'ResNet18 Improved' in model_name:\n","            model = model_class(num_classes, pretrained=False)\n","        else:\n","            model = model_class(num_classes)\n","        checkpoint_path = os.path.join(SAVE_DIR, checkpoint_name)\n","        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n","        model = model.to(device)\n","        model.eval()\n","        models_info.append((model, model_name))\n","    occlusion_sensitivity_analysis(\n","        models_info=models_info,\n","        device=device,\n","        loader=sample_loader,\n","        output_dir=output_dir,\n","        class_names=class_names,\n","        num_examples=num_samples,\n","        patch_size=patch_size,\n","        stride=stride,\n","        baseline_value=baseline_tensor\n","    )\n","\n","\n","if __name__ == '__main__':\n","    main()\n"]}]}