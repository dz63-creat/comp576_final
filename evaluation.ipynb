{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOrXUKoC8A/2BAYcO3Dy2xL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nA1b1KXYS2pr","executionInfo":{"status":"ok","timestamp":1765777841786,"user_tz":360,"elapsed":745638,"user":{"displayName":"Fangzhou Du","userId":"14946545626395308065"}},"outputId":"a95be55b-dc34-4bae-cca7-2cf6b7cdd8cb"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Using Colab cache for faster access to the 'plantdisease' dataset.\n"]},{"output_type":"stream","name":"stderr","text":["Inference: 100%|██████████| 65/65 [03:27<00:00,  3.19s/it]\n","Inference: 100%|██████████| 65/65 [03:22<00:00,  3.12s/it]\n","Inference: 100%|██████████| 65/65 [03:00<00:00,  2.77s/it]\n","Inference: 100%|██████████| 65/65 [02:30<00:00,  2.32s/it]\n","Inference: 100%|██████████| 65/65 [02:25<00:00,  2.24s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Model: best_baseline_resnet18.pth\n","Accuracy: 99.95%\n","Precision: 0.9995\n","Recall: 0.9996\n","Latency (BS=1): 107.93 ms\n","Params: 11.18 M\n","Size: 42.66 MB\n","Model: best_improved_resnet18plant.pth\n","Accuracy: 99.85%\n","Precision: 0.9984\n","Recall: 0.9989\n","Latency (BS=1): 103.49 ms\n","Params: 11.44 M\n","Size: 43.65 MB\n","Model: best_cnn_model.pth\n","Accuracy: 98.40%\n","Precision: 0.9826\n","Recall: 0.9827\n","Latency (BS=1): 53.64 ms\n","Params: 0.46 M\n","Size: 1.75 MB\n","Model: best_customcnn_distilled.pth\n","Accuracy: 98.50%\n","Precision: 0.9832\n","Recall: 0.9726\n","Latency (BS=1): 64.16 ms\n","Params: 0.46 M\n","Size: 1.75 MB\n","Model: best_customcnn_distilled_real_baseline.pth\n","Accuracy: 98.25%\n","Precision: 0.9805\n","Recall: 0.9697\n","Latency (BS=1): 59.69 ms\n","Params: 0.46 M\n","Size: 1.75 MB\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from torchvision import transforms, models, datasets\n","from torchvision.datasets import ImageFolder\n","import os\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import time\n","from sklearn.metrics import classification_report, confusion_matrix\n","from tqdm import tqdm\n","import kagglehub\n","\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","NUM_WORKERS = 2\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","SAVE_DIR = '/content/drive/MyDrive/comp576final'\n","\n","path = kagglehub.dataset_download(\"emmarex/plantdisease\")\n","DATA_DIR = os.path.join(path, 'PlantVillage')\n","\n","transform_eval = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","class DatasetWrapper(Dataset):\n","    def __init__(self, subset, transform=None):\n","        self.subset = subset\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        x, y = self.subset[index]\n","        if self.transform:\n","            x = self.transform(x)\n","        return x, y\n","\n","    def __len__(self):\n","        return len(self.subset)\n","\n","def load_test_data(data_dir, save_dir):\n","    raw_dataset = ImageFolder(root=data_dir)\n","    classes = raw_dataset.classes\n","    split_path = os.path.join(save_dir, 'data_split_indices.pth')\n","    split_indices = torch.load(split_path)\n","    test_indices = split_indices['test_indices']\n","    test_subset = Subset(raw_dataset, test_indices)\n","    test_dataset = DatasetWrapper(test_subset, transform=transform_eval)\n","    use_pin_memory = (device.type == 'cuda')\n","    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n","                           num_workers=NUM_WORKERS, pin_memory=use_pin_memory)\n","    return test_loader, test_dataset, classes\n","\n","def create_resnet18_model(num_classes, pretrained=False):\n","    model = models.resnet18(weights=None) # We load weights later\n","    num_ftrs = model.fc.in_features\n","    model.fc = nn.Linear(num_ftrs, num_classes)\n","    return model\n","\n","class ResNet18Plant(nn.Module):\n","    def __init__(self, num_classes, pretrained=False):\n","        super().__init__()\n","        backbone = models.resnet18(weights=None)\n","        self.features = nn.Sequential(*list(backbone.children())[:-2])\n","        in_channels = 512\n","        self.classifier = nn.Sequential(\n","            nn.Linear(in_channels * 2, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        gap = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)\n","        gmp = F.adaptive_max_pool2d(x, 1).view(x.size(0), -1)\n","        feat = torch.cat([gap, gmp], dim=1)\n","        out = self.classifier(feat)\n","        return out\n","\n","class PlantCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n","        )\n","        self.global_pool = nn.AdaptiveAvgPool2d(1)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(256, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.global_pool(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return out\n","\n","def measure_latency(model, device, dataset, num_samples=100):\n","    model.eval()\n","    subset_indices = range(min(num_samples, len(dataset)))\n","    subset = Subset(dataset, subset_indices)\n","    latency_loader = DataLoader(subset, batch_size=1, shuffle=False,\n","                              num_workers=1, pin_memory=False)\n","    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n","    for _ in range(10):\n","        with torch.no_grad():\n","            _ = model(dummy_input)\n","    total_time = 0\n","    count = 0\n","\n","    with torch.no_grad():\n","        for inputs, _ in latency_loader:\n","            inputs = inputs.to(device)\n","\n","            if device.type == 'cuda':\n","                torch.cuda.synchronize()\n","            start_time = time.time()\n","\n","            _ = model(inputs)\n","\n","            if device.type == 'cuda':\n","                torch.cuda.synchronize()\n","            end_time = time.time()\n","\n","            total_time += (end_time - start_time)\n","            count += 1\n","\n","    avg_latency = (total_time / count) * 1000 # ms\n","    return avg_latency\n","\n","def evaluate_model(model_path, model, dataloader, dataset, device, classes):\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.to(device)\n","    model.eval()\n","    latency_ms = measure_latency(model, device, dataset)\n","\n","    all_preds = []\n","    all_labels = []\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(dataloader, desc=\"Inference\"):\n","            inputs = inputs.to(device)\n","            labels = list(labels.numpy())\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels)\n","            total_samples += inputs.size(0)\n","\n","    report = classification_report(all_labels, all_preds, target_names=classes, output_dict=True)\n","    cm = confusion_matrix(all_labels, all_preds)\n","    precision = report['macro avg']['precision']\n","    recall = report['macro avg']['recall']\n","    accuracy = report['accuracy']\n","    return {\n","        'model_name': os.path.basename(model_path),\n","        'inference_time_ms': latency_ms,\n","        'precision': precision,\n","        'recall': recall,\n","        'accuracy': accuracy,\n","        'confusion_matrix': cm,\n","        'all_labels': all_labels,\n","        'all_preds': all_preds,\n","        'full_report': report\n","    }\n","\n","def plot_confusion_matrix(cm, classes, model_name):\n","    plt.figure(figsize=(12, 10))\n","    annot = True if len(classes) < 20 else False\n","    sns.heatmap(cm, annot=annot, fmt='d', cmap='Blues',\n","                xticklabels=classes if len(classes) < 50 else [],\n","                yticklabels=classes if len(classes) < 50 else [])\n","    plt.title(f'Confusion Matrix - {model_name}')\n","    plt.ylabel('True Label')\n","    plt.xlabel('Predicted Label')\n","    plt.tight_layout()\n","    save_path = os.path.join(SAVE_DIR, f'confusion_matrix_{model_name}.png')\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","def plot_samples(model, dataloader, classes, model_name, device):\n","    model.eval()\n","    images, labels = next(iter(dataloader))\n","    indices = random.sample(range(len(images)), 5)\n","    selected_images = images[indices]\n","    selected_labels = labels[indices]\n","\n","    with torch.no_grad():\n","        outputs = model(selected_images.to(device))\n","        _, preds = torch.max(outputs, 1)\n","\n","    plt.figure(figsize=(15, 5))\n","    for i in range(5):\n","        ax = plt.subplot(1, 5, i + 1)\n","        img = selected_images[i].cpu().permute(1, 2, 0).numpy()\n","        mean = np.array([0.485, 0.456, 0.406])\n","        std = np.array([0.229, 0.224, 0.225])\n","        img = std * img + mean\n","        img = np.clip(img, 0, 1)\n","        true_name = classes[selected_labels[i]]\n","        pred_name = classes[preds[i]]\n","        color = 'green' if true_name == pred_name else 'red'\n","        plt.imshow(img)\n","        plt.title(f\"True: {true_name}\\nPred: {pred_name}\", color=color, fontsize=10)\n","        plt.axis('off')\n","\n","    plt.suptitle(f'Sample Predictions - {model_name}', fontsize=16)\n","    plt.tight_layout()\n","    save_path = os.path.join(SAVE_DIR, f'samples_{model_name}.png')\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","def main():\n","    test_loader, test_dataset, classes = load_test_data(DATA_DIR, SAVE_DIR)\n","    num_classes = len(classes)\n","\n","    models_to_eval = [\n","        {\n","            'path': 'best_baseline_resnet18.pth',\n","            'model': create_resnet18_model(num_classes)\n","        },\n","        {\n","            'path': 'best_improved_resnet18plant.pth',\n","            'model': ResNet18Plant(num_classes)\n","        },\n","        {\n","            'path': 'best_cnn_model.pth',\n","            'model': PlantCNN(num_classes)\n","        },\n","        {\n","            'path': 'best_customcnn_distilled.pth',\n","            'model': PlantCNN(num_classes)\n","        },\n","        {\n","            'path': 'best_customcnn_distilled_real_baseline.pth',\n","            'model': PlantCNN(num_classes)\n","        }\n","    ]\n","\n","    results = []\n","\n","    for item in models_to_eval:\n","        full_path = os.path.join(SAVE_DIR, item['path'])\n","\n","        res = evaluate_model(full_path, item['model'], test_loader, test_dataset, device, classes)\n","        if res:\n","            results.append(res)\n","\n","            plot_confusion_matrix(res['confusion_matrix'], classes, res['model_name'])\n","            plot_samples(item['model'], test_loader, classes, res['model_name'], device)\n","\n","    for res in results:\n","        # Calculate params and size to display\n","        model_name = res['model_name']\n","        matching_item = next(item for item in models_to_eval if item['path'] == model_name)\n","        model = matching_item['model']\n","        total_params = sum(p.numel() for p in model.parameters())\n","        param_millions = total_params / 1e6\n","        size_mb = (total_params * 4) / (1024 * 1024)\n","\n","        print(f\"Model: {res['model_name']}\")\n","        print(f\"Accuracy: {res['accuracy']*100:.2f}%\")\n","        print(f\"Precision: {res['precision']:.4f}\")\n","        print(f\"Recall: {res['recall']:.4f}\")\n","        print(f\"Latency (BS=1): {res['inference_time_ms']:.2f} ms\")\n","        print(f\"Params: {param_millions:.2f} M\")\n","        print(f\"Size: {size_mb:.2f} MB\")\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}